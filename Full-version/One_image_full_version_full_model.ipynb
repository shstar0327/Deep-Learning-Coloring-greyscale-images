{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras:  2.0.8\n"
     ]
    }
   ],
   "source": [
    "print('keras: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "218497024/219055592 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Get images\n",
    "image_raw = img_to_array(load_img('dog.jpg'))\n",
    "image_raw = np.array(image_raw, dtype=float)\n",
    "#Load weights\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 65536 into shape (1,312,312,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d7498b770b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m312\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m312\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m312\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m312\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 65536 into shape (1,312,312,1)"
     ]
    }
   ],
   "source": [
    "X = rgb2lab(1.0/255*image_raw)[:,:,0]\n",
    "Y = rgb2lab(1.0/255*image_raw)[:,:,1:]\n",
    "Y /= 128\n",
    "X = X.reshape(1, 312, 312, 1)\n",
    "Y = Y.reshape(1, 312, 312, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_global.shape)\n",
    "\n",
    "def conv_stack(, filters, strides):\n",
    "    for i in strides:\n",
    "        model.add(Conv2D(filters, (3, 3), strides=i, activation='relu', dilation_rate=d, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "#Add inception embedding\n",
    "img_path = 'dog.jpg'\n",
    "img = image.load_img(img_path, target_size=(299, 299))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = preprocess_input(img)\n",
    "embed = inception.predict(img)\n",
    "\n",
    "#Encoder\n",
    "encoder = Sequential() \n",
    "encoder.add(InputLayer(input_shape=(None, None, 1)))\n",
    "conv_stack(encoder, 64, [2])(X)\n",
    "conv_stack(encoder, 128, [1, 2])\n",
    "conv_stack(encoder, 256, [1, 2])\n",
    "conv_stack(encoder, 512, [1, 1])\n",
    "conv_stack(encoder, 256, [1])\n",
    "encoder_output = conv_stack(128, 1, [1])\n",
    "\n",
    "#Fusion\n",
    "# y_mid: (None, 256, 28, 28)\n",
    "fusion = Sequential() \n",
    "fusion.add(InputLayer(input_shape=(32, 32, 1)))(embed)\n",
    "fusion.add(RepeatVector(28 * 28)) # shape: (None, 28*28, 256)\n",
    "fusion.add(Permute((2, 1))) # shape: (None, 256, 28*28)\n",
    "fusion_embed = fusion.add(Reshape(256, 28, 28)) # shape: (None, 256, 28, 28)\n",
    "y_concat = Merge(layers=[fusion_embed, encoder_output], mode='concat', concat_axis=1) # (None, 512, 28, 28)\n",
    "fusion_output = Conv2D(256, 1, 1, activation='relu')(y_concat) # (None, 256, 28, 28) and Eq. (5)\n",
    "\n",
    "#Decoder\n",
    "decoder = Sequential() \n",
    "decoder.add(InputLayer(input_shape=(fusion_output.shape)))(fusion_output)\n",
    "decoder.add(UpSampling2D((2, 2)))(y_fusion)\n",
    "conv_stack(decoder, 64, [1, 1])\n",
    "decoder.add(UpSampling2D((2, 2)))\n",
    "conv_stack(decoder, 32, [1])\n",
    "decoder.add(Conv2D(2, (3, 3), activation='tanh'))\n",
    "out = decoder.add(UpSampling2D((2, 2)))\n",
    "\n",
    "model = Model(inputs=[embed, X], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finish model\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "model.fit(x=X, \n",
    "    y=Y,\n",
    "    batch_size=1,\n",
    "    epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(decoder.evaluate(X, Y, batch_size=1))\n",
    "output = decoder.predict(X)\n",
    "output *= 128\n",
    "# Output colorizations\n",
    "cur = np.zeros((300, 300, 3))\n",
    "cur[:,:,0] = X[0][:,:,0]\n",
    "cur[:,:,1:] = output[0]\n",
    "imsave(\"img_result.png\", lab2rgb(cur))\n",
    "imsave(\"img_gray_version.png\", rgb2gray(lab2rgb(cur)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
