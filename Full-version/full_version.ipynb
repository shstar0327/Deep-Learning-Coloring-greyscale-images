{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, InputLayer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.engine import Layer\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get images\n",
    "X = []\n",
    "for filename in os.listdir('/color_300/Train/'):\n",
    "    X.append(img_to_array(load_img('/color_300/Train/'+filename)))\n",
    "X = np.array(X, dtype=float)\n",
    "\n",
    "# Set up train and test data\n",
    "split = int(0.95*len(X))\n",
    "Xtrain = X[:split]\n",
    "Xtrain = 1.0/255*Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_image_for_inception(input_tensor):\n",
    "    \"\"\"\n",
    "    Pre-processes an image tensor ``(int8, range [0, 255])``\n",
    "    to be fed into inception ``(float32, range [-1, +1])``\n",
    "    :param input_tensor:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = tf.cast(input_tensor, dtype=tf.float32)\n",
    "    res = 2 * res / 255 - 1\n",
    "    res = tf.reshape(res, [-1, 300, 300, 3])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_operations(self, examples_per_record):\n",
    "        \"\"\"\n",
    "        Create the operations to read images from the queue and\n",
    "        extract inception features\n",
    "        :return: a tuple containing all these operations\n",
    "        \"\"\"\n",
    "        # Create the queue operations\n",
    "        image_key, image_tensor, _ = \\\n",
    "            queue_single_images_from_folder(self.inputs_dir)\n",
    "\n",
    "        # Build Inception Resnet v2 operations using the image as input\n",
    "        # - from rgb to grayscale to loose the color information\n",
    "        # - from grayscale to rgb just to have 3 identical channels\n",
    "        # - from a [0, 255] int8 range to [-1,+1] float32\n",
    "        # - feed the image into inception and get the embedding\n",
    "        img_for_inception = tf.image.rgb_to_grayscale(image_tensor)\n",
    "        img_for_inception = tf.image.grayscale_to_rgb(img_for_inception)\n",
    "        img_for_inception = prepare_image_for_inception(img_for_inception)\n",
    "        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "            input_embedding, _ = inception_resnet_v2(img_for_inception,\n",
    "                                                     is_training=False)\n",
    "\n",
    "        operations = image_key, image_tensor, input_embedding\n",
    "\n",
    "        return batch_operations(operations, examples_per_record)\n",
    "\n",
    "    def _run_session(self, sess, operations, examples_per_record):\n",
    "        \"\"\"\n",
    "        Run the whole reading -> extracting features -> writing to records\n",
    "        pipeline in a TensorFlow session\n",
    "        :param sess:\n",
    "        :param operations:\n",
    "        :param examples_per_record:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Coordinate the loading of image files.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        start_time = time.time()\n",
    "        self._examples_count = 0\n",
    "\n",
    "        # These are the only lines where something happens:\n",
    "        # we execute the operations to get the image, compute the\n",
    "        # embedding and write everything in the TFRecord\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                self._write_record(examples_per_record, operations, sess)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            # The string_input_producer queue ran out of strings\n",
    "            pass\n",
    "        finally:\n",
    "            # Ask the threads (filename queue) to stop.\n",
    "            coord.request_stop()\n",
    "            print('Finished writing {} images in {:.2f}s'\n",
    "                  .format(self._examples_count, time.time() - start_time))\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "\n",
    "def batch_all(self, examples_per_record):\n",
    "        operations = self._create_operations(examples_per_record)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self._initialize_session(sess)\n",
    "            self._run_session(sess, operations, examples_per_record)\n",
    "\n",
    "    def _initialize_session(self, sess):\n",
    "        \"\"\"\n",
    "        Initialize a new session to run the operations\n",
    "        :param sess:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the the variables that we introduced (like queues etc.)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        # Restore the weights from Inception\n",
    "        # (do not call a global/local variable initializer after this call)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, self.checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FusionLayer(Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        imgs, embs = inputs\n",
    "        reshaped_shape = imgs.shape[:3].concatenate(embs.shape[1])\n",
    "        embs = K.repeat(embs, imgs.shape[1] * imgs.shape[2])\n",
    "        embs = K.reshape(embs, reshaped_shape)\n",
    "        return K.concatenate([imgs, embs], axis=3)\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        # Must have 2 tensors as input\n",
    "        assert input_shapes and len(input_shapes) == 2\n",
    "        imgs_shape, embs_shape = input_shapes\n",
    "\n",
    "        # The batch size of the two tensors must match\n",
    "        assert imgs_shape[0] == embs_shape[0]\n",
    "\n",
    "        # (batch_size, width, height, embedding_len + depth)\n",
    "        return imgs_shape[:3] + (imgs_shape[3] + embs_shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Colorization:\n",
    "    def __init__(self, depth_after_fusion):\n",
    "        self.encoder = _build_encoder()\n",
    "        self.fusion = FusionLayer()\n",
    "        self.after_fusion = Conv2D(\n",
    "            depth_after_fusion, (1, 1), activation='relu')\n",
    "        self.decoder = _build_decoder(depth_after_fusion)\n",
    "\n",
    "    def build(self, img_l, img_emb):\n",
    "        img_enc = self.encoder(img_l)\n",
    "\n",
    "        fusion = self.fusion([img_enc, img_emb])\n",
    "        fusion = self.after_fusion(fusion)\n",
    "\n",
    "        return self.decoder(fusion)\n",
    "\n",
    "def conv_stack(filters, d, strides):\n",
    "    for i in strides:\n",
    "        model.add(Conv2D(filters, (3, 3), strides=i, activation='relu', dilation_rate=d, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "def _build_encoder():\n",
    "    model = Sequential(name='encoder')\n",
    "    model.add(InputLayer(input_shape=(None, None, 1)))\n",
    "    conv_stack(64, 1, [2])\n",
    "    conv_stack(128, 1, [1, 2])\n",
    "    conv_stack(256, 1, [1, 2])\n",
    "    conv_stack(512, 1, [1, 1])\n",
    "    conv_stack(256, 1, [1])\n",
    "    conv_stack(128, 1, [1])\n",
    "    return model\n",
    "\n",
    "def _build_decoder(encoding_depth):\n",
    "    model = Sequential(name='decoder')\n",
    "    model.add(InputLayer(input_shape=(None, None, encoding_depth)))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    conv_stack(64, 1, [1, 1])\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    conv_stack(32, 1, [1])\n",
    "    model.add(Conv2D(2, (3, 3), activation='tanh'))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    return model\n",
    "    \n",
    "model.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Generate training data\n",
    "batch_size = 50\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)\n",
    "\n",
    "# Train model      \n",
    "tensorboard = TensorBoard(log_dir=\"/output/{}\")\n",
    "model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=2, samples_per_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test images\n",
    "Xtest = rgb2lab(1.0/255*X[split:])[:,:,:,0]\n",
    "Xtest = Xtest.reshape(Xtest.shape+(1,))\n",
    "Ytest = rgb2lab(1.0/255*X[split:])[:,:,:,1:]\n",
    "Ytest = Ytest / 128\n",
    "print(model.evaluate(Xtest, Ytest, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir('/color_300/Test/'):\n",
    "\tcolor_me.append(img_to_array(load_img('/color_300/Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "# Test model\n",
    "output = model.predict(color_me)\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "\tcur = np.zeros((300, 300, 3))\n",
    "\tcur[:,:,0] = Xtest[i][:,:,0]\n",
    "\tcur[:,:,1:] = output[i]\n",
    "\timsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
